{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attack_method.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxf1UF6SgrZ4"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Attack_detection/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFkwmdP7s1Qv"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.utils.data as data\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms\n",
        "from matplotlib import colors\n",
        "\n",
        "%cd /content/drive/MyDrive/Attack_detection/Fast-SCNN-pytorch/Fast-SCNN-pytorch/\n",
        "from data_loader import CitySegmentation\n",
        "from models.fast_scnn import get_fast_scnn\n",
        "from utils.visualize import get_color_pallete\n",
        "\n",
        "from PIL import Image\n",
        "import tqdm\n",
        "\n",
        "%cd /content/drive/MyDrive/Attack_detection/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn25TQGPSfIf"
      },
      "source": [
        "LABEL_NAMES = np.asarray([\n",
        "    'Road', 'Sidewalk', 'Building', 'Wall', 'Fence', 'Pole', 'TrafficLight',\n",
        "    'TrafficSign', 'Vegetation', 'Terrain', 'Sky', 'Person', 'Rider',\n",
        "    'Car', 'Truck', 'Bus', 'Train', 'Motorcycle', 'Bicycle', 'Unlabeled'\n",
        "])\n",
        "\n",
        "LABEL_COLORS = [\n",
        "    (128, 64, 128), (244, 35, 232), (70, 70, 70), (102, 102, 156),\n",
        "    (190, 153, 153), (153, 153, 153), (250, 170, 30), (220, 220, 0),\n",
        "    (107, 142, 35), (152, 251, 152), (70, 130, 180), (220, 20, 60),\n",
        "    (255, 0, 0), (0, 142, 142), (0, 0, 70), (0, 60, 100), (0, 80, 100),\n",
        "    (0, 0, 230), (119, 11, 32), (0, 0, 142)]\n",
        "\n",
        "VALID_CLASSES = [7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33, -1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33sNIkmAsLRQ"
      },
      "source": [
        "def create_target(device,model,img, target_class):\n",
        "    model.eval() #prepres model for evaluation, PYTORCH ONLY!\n",
        "    pred_img = model(img)[0]\n",
        "    y_target = pred_img.argmax(1).type(torch.LongTensor).to(device) #prediction with max over the pixel, AXIS 1\n",
        "    o_mask = (y_target == target_class).squeeze() #MASK WITH 1 WHERE THE TARGET CLASS\n",
        "    bg_mask = ~o_mask #MASK WITH 1 WHERE THE TARGET CLASS\n",
        "    o_none_zero = torch.nonzero(o_mask) #CREATE NONZERO\n",
        "    bg_none_zero = torch.nonzero(bg_mask)#CREATE NONZERO \n",
        "    count = 0\n",
        "    for i, j  in o_none_zero: #CREATE LONG LIST OF DUPLICATE, VECTOR TO SUB FROM BG NONZERO vectorized style\n",
        "        i_j_tensor = torch.tensor([[i, j]]).repeat(bg_none_zero.size(0), 1).to(device) #vectorized, repeat by background size.\n",
        "        idx = ((i_j_tensor - bg_none_zero)**2).sum(1).argmin(0) #idx of neighbor\n",
        "        i_new, j_new = bg_none_zero[idx][0], bg_none_zero[idx][1]\n",
        "        y_target[0, i, j] = y_target[0, i_new, j_new]\n",
        "        if count % 5000 == 0 and count > 0: #optimization, avoid buffer overflows\n",
        "            print(count)\n",
        "        count+=1\n",
        "        del i_j_tensor\n",
        "    return y_target #picture without the target class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ugkuKD3hJm3"
      },
      "source": [
        "def image_dependent_attack_pgd(device,model,img, eps, alpha, y_target, target_class, max_iter, plot):\n",
        "    model.eval() #prepres model for evaluation, PYTORCH ONLY!\n",
        "    pertubation = torch.zeros_like(img) \n",
        "    y_target=y_target.to(device)\n",
        "    for i in range(max_iter):\n",
        "        pertubation = Variable(pertubation, requires_grad=True)\n",
        "        selected_model_outputs = model(img+pertubation)[0]\n",
        "        outputs = F.softmax(selected_model_outputs, dim=1) #max over each pixel, dim = 1\n",
        "        loss = F.cross_entropy(outputs, y_target) #ce is a measure between two histograms, not 2 scalars. (be carefull when everything besides 1 scalars is nonzero). in tf requires one hot.\n",
        "        print(f'Iter {i} loss is: {loss.item()}')\n",
        "        loss.backward()\n",
        "        gradient =  pertubation.grad.data.sign()\n",
        "        pertubation = torch.clamp(pertubation - alpha*gradient, -eps, eps) #pgd style clapping.\n",
        "\n",
        "        predicted_image = model(img+pertubation)[0].argmax(1).squeeze(0)#in the depth (dim 3 ) the size is the number of classes of segementations. two dim in the end by argmax.\n",
        "        predicted_image = predicted_image.cpu().detach()\n",
        "        if plot and i%10 == 0:\n",
        "            plt.imshow(predicted_image)\n",
        "            plt.title('Iteration number {}'.format(i))\n",
        "            plt.show()\n",
        "\n",
        "        pixels_left = (predicted_image==target_class).sum().numpy() #how much pixels is removed from the output. the sum 0 is the best. sort of accuracy.\n",
        "        print(f'Class pixel left {pixels_left}')\n",
        "    return (img+pertubation).detach()\n",
        "\n",
        "\n",
        "def image_dependent_attack_weighted_avg(device,model,img, eps, alpha, w, y_target, target_class, max_iter, plot):\n",
        "    model.eval()#prepres model for evaluation, PYTORCH ONLY!\n",
        "    pertubation = torch.zeros_like(img)\n",
        "    y_target=y_target.to(device)\n",
        "    for i in range(max_iter):\n",
        "        pertubation = Variable(pertubation, requires_grad=True)\n",
        "        selected_model_outputs = model(img+pertubation)[0]\n",
        "        outputs = F.softmax(selected_model_outputs, dim=1) #max over each pixel, dim = 1\n",
        "\n",
        "        preds = outputs.argmax(1).to(device)\n",
        "        target_mask = (preds == target_class).to(device)\n",
        "        bg_mask = (preds != target_class).to(device)\n",
        "        mask = (((preds == y_target) & (outputs.max(1)[0].to(device) < 0.85).to(device)) & target_mask) | ((preds != y_target) & target_mask) #0.85 is the confidence score.    \n",
        "        o_pixels = torch.masked_select(outputs, mask).view(-1,  19)\n",
        "        o_preds = torch.masked_select(y_target, mask).view(-1)\n",
        "        o_loss = w*F.cross_entropy(o_pixels, o_preds, reduction='sum')\n",
        "        \n",
        "        mask = (((preds == y_target) & (outputs.max(1)[0].to(device) < 0.85).to(device)) & bg_mask) | ((preds != y_target) & bg_mask)  #0.85 is the confidence score.    \n",
        "        bg_pixels = torch.masked_select(outputs, mask).view(-1, 19)\n",
        "        bg_preds = torch.masked_select(y_target, mask).view(-1)\n",
        "        bg_loss = (1-w)*F.cross_entropy(bg_pixels, bg_preds, reduction='sum')\n",
        "        \n",
        "        loss = 1/(bg_preds.size(0)+o_preds.size(0))*(bg_loss + o_loss)\n",
        "        loss.backward()\n",
        "\n",
        "        gradient =  torch.ge(pertubation.grad.data, 0)\n",
        "        gradient = (gradient.float() - 0.5) * 2\n",
        "\n",
        "        # Normalizing the gradient to the same space of image\n",
        "        gradient[0][0] = (gradient[0][0])/(transform_norm_x_std)\n",
        "        gradient[0][1] = (gradient[0][1])/(transform_norm_y_std)\n",
        "        gradient[0][2] = (gradient[0][2])/(transform_norm_z_std)\n",
        "        \n",
        "        pertubation = torch.clamp(pertubation - alpha*gradient, -eps, eps) #pgd style clapping.\n",
        "        predicted_image = model(img+pertubation)[0].argmax(1).squeeze(0)#in the depth (dim 3 ) the size is the number of classes of segementations. two dim in the end by argmax.\n",
        "        predicted_image = predicted_image.cpu().detach()\n",
        "\n",
        "        if plot and i%5 == 0:\n",
        "            plt.imshow(model(img+pertubation)[0].argmax(1).cpu().detach())\n",
        "            plt.title('iteration number {}'.format(i))\n",
        "            plt.show()\n",
        "            \n",
        "        pixels_left = (predicted_image==target_class).sum().numpy() #how much pixels is removed from the output. the sum 0 is the best. sort of accuracy.\n",
        "        print(f'Class pixel left {pixels_left}')\n",
        "    return (img+pertubation).detach()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1VxEWcAMlw-"
      },
      "source": [
        "def get_map():\n",
        "    return np.array(list(zip(range(len(VALID_CLASSES)), VALID_CLASSES, LABEL_NAMES, LABEL_COLORS)))\n",
        "\n",
        "MY_MAP = get_map()\n",
        "\n",
        "def get_labels(lbls):\n",
        "    ret = {'colors': [], 'names': []}\n",
        "    for lbl in lbls:\n",
        "        ret['colors'].append(MY_MAP[lbl][-1])\n",
        "        ret['names'].append(MY_MAP[lbl][-2])\n",
        "    ret['colors'] = np.asarray(ret['colors']).reshape(-1, 1, 3)\n",
        "    return ret\n",
        "\n",
        "def to_rgba(rgb, one=True):\n",
        "    return tuple([x * 1.0 / 255 for x in rgb]) + (1. if one else 0.,)\n",
        "\n",
        "cmaplist = [to_rgba(x[3]) for x in MY_MAP] # [(x[0], to_rgba(x[3])) for x in MY_MAP]\n",
        "cmap = colors.ListedColormap(cmaplist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I4npkw0MlAx"
      },
      "source": [
        "def fix_mask(mask):\n",
        "    get_id = np.vectorize(lambda x: MY_MAP[x][1])\n",
        "    return get_id(mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSHMlWG9JON8"
      },
      "source": [
        "def save_tensor(device,model, img, mask,image_idx,pgd_attack=True,eps = 8.0 / 255, max_iter=100):\n",
        "  print(\"saving tensor for img idx: \"+str(image_idx))\n",
        "  model.eval()\n",
        "  img_in_device = img.to(device)\n",
        "  mask_fixed = fix_mask(mask.squeeze())\n",
        "  clean_seg_pred = model(img_in_device)[0].argmax(1).cpu()\n",
        "  unique_labels = np.array(list(set(np.unique(mask_fixed)).union(set(np.unique(clean_seg_pred)))))\n",
        "  target_class = 13 if 13 in unique_labels else unique_labels[-2] #Car target\n",
        "  #labels = get_labels(unique_labels)\n",
        "  print('Attacking target class: {} - {}'.format(target_class, LABEL_NAMES[target_class]))\n",
        "  y_target = create_target(device,model,img_in_device, target_class=target_class) #in tf h,w,c , in pytorch c,h,w\n",
        "  if pgd_attack == True:\n",
        "    attack = image_dependent_attack_pgd(device,model,img=img_in_device, eps=eps, alpha=0.0001, y_target=y_target, target_class=target_class, max_iter=max_iter, plot=False)\n",
        "  else:\n",
        "    attack = image_dependent_attack_weighted_avg(device,model,img=img_in_device, eps=eps, alpha=0.0001, w = 0.1, y_target=y_target, target_class=target_class, max_iter=max_iter, plot=False)\n",
        "  perturbed_img = attack.cpu().squeeze()                       \n",
        "  perturbed_pred = model(attack.to(device))[0].argmax(1).cpu()\n",
        "  dest_dir = \"./SCNN_attack_output_alphas\"\n",
        "  dest = dest_dir+\"/{0}_{1}.pt\"\n",
        "  img = img.squeeze()\n",
        "  torch.save(img, dest.format(\"img\", image_idx))\n",
        "  torch.save(mask, dest.format(\"real_mask\", image_idx))\n",
        "  torch.save(clean_seg_pred, dest.format(\"mask\", image_idx))\n",
        "  torch.save(perturbed_img, dest.format(\"perturbed_img\", image_idx))\n",
        "  torch.save(perturbed_pred, dest.format(\"perturbed_mask\", image_idx))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2WgpuFit3Ar"
      },
      "source": [
        "transform_norm_x_std = 0.229\n",
        "transform_norm_y_std = 0.224\n",
        "transform_norm_z_std = 0.225\n",
        "\n",
        "transform_norm_x_avg = .485\n",
        "transform_norm_y_avg = .456\n",
        "transform_norm_z_avg = .406\n",
        "\n",
        "input_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([transform_norm_x_avg, transform_norm_y_avg, transform_norm_z_avg], [transform_norm_x_std,transform_norm_y_std ,transform_norm_z_std ]),])\n",
        "\n",
        "dataset_source = './dataset_Cityscapes/'\n",
        "split_type = 'val'\n",
        "\n",
        "val_dataset = CitySegmentation(root=dataset_source, \n",
        "                               split=split_type, \n",
        "                               mode='testval',\n",
        "                               transform=input_transform)\n",
        "\n",
        "val_loader = data.DataLoader(dataset=val_dataset,\n",
        "                                  batch_size=1,\n",
        "                                  shuffle=False)\n",
        "\n",
        "required_path_for_images = dataset_source+split_type\n",
        "if not os.path.isdir(required_path_for_images):\n",
        "  os.mkdir(required_path_for_images)\n",
        "  print(\"Directory '%s' created\" %required_path_for_images)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfZT-bRaJSZP"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = get_fast_scnn('citys', pretrained=True, root='./Fast-SCNN-pytorch/Fast-SCNN-pytorch/weights', map_cpu=False).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUfytzf2okvZ"
      },
      "source": [
        "pgd_attack = True # if True -> pgd, if False -> weighted.\n",
        "eps = 8.0 / 255\n",
        "max_iter_in_attack=100\n",
        "\n",
        "run_tensor_save = True\n",
        "if run_tensor_save == True:\n",
        "  for i, obj in enumerate(val_loader):\n",
        "    img, mask = obj[0], obj[1]\n",
        "    save_tensor(device,model, img, mask,i,pgd_attack,eps,max_iter_in_attack)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISEsCHMhb-Y3"
      },
      "source": [
        "foreground_classes = [5, 6, 7, 11, 13, 14, 15, 17, 18]\n",
        "\n",
        "def class_to_rgba(lbl, trans=False):\n",
        "  return tuple([x / 255 for x in MY_MAP[lbl][3]]) + (1. if not trans else 0.,)\n",
        "\n",
        "def vec_to_rgba(vec):\n",
        "  ret = []\n",
        "  for i in range(vec.shape[0]):\n",
        "    ret.append([class_to_rgba(vec[i][j], vec[i][j] in foreground_classes) for j in range(vec.shape[1])])\n",
        "  return np.array(ret)\n",
        "\n",
        "def save_fig(path, img, mask=None, alpha=0.9):\n",
        "  \"\"\"\n",
        "  Plots the image and overlay and saves it into path\n",
        "  Arguments:\n",
        "  path -- the path in which to save the image\n",
        "  img -- the image tensor, need cast to HWC\n",
        "  Keyword arguments:\n",
        "  mask -- if not None, should be a tensor of the segmentation mask, and plots it on top of the image, need cast CHW to HWC\n",
        "  alpha -- the transparency of the overlay\n",
        "\n",
        "  \"\"\"\n",
        "  plt.axis(\"off\")\n",
        "\n",
        "  print(\"image shape:\"+str(img.shape))\n",
        "  if img.shape[0] <= 3: #  sometimes need to cast permute(1, 2, 0) from CHW to HWC. e.g. [3/1,1024,2048] - > [1024,2048,3/1]\n",
        "    img = img.permute(1, 2, 0)\n",
        "    print(\"image shape after permute:\"+str(img.shape))\n",
        "\n",
        "  if mask is not None:\n",
        "      print(\"mask shape:\"+str(mask.shape))\n",
        "      if mask.shape[0] <= 3:  #  sometimes need to cast permute(1, 2, 0) from CHW to HWC. e.g. [3/1,1024,2048] - > [1024,2048,3/1]\n",
        "        mask = mask.permute(1, 2, 0)\n",
        "        print(\"mask shape after permute:\"+str(mask.shape))\n",
        "\n",
        "\n",
        "  mu = [transform_norm_x_avg, transform_norm_y_avg, transform_norm_z_avg]\n",
        "  std = [transform_norm_x_std,transform_norm_y_std ,transform_norm_z_std]\n",
        "  for dim in range(3):\n",
        "    img[:, :, dim] = img[:, :, dim]*std[dim]+mu[dim]\n",
        "\n",
        "  if mask is not None:\n",
        "    mask = vec_to_rgba(mask)\n",
        "    mask = mask[:,:,:3]\n",
        "    img = img*alpha + mask*(1-alpha)\n",
        "  im = np.array(img)\n",
        "  im = Image.fromarray((im * 255).astype(np.uint8))\n",
        "  im.save(path)\n",
        "\n",
        "def save_img_from_tensor(i, alpha):\n",
        "  \"\"\"\n",
        "  Reads tensors and saves as image given the alpha\n",
        "  \"\"\"\n",
        "  src_dest_dir = \"./SCNN_attack_output_alphas\"\n",
        "  dest = src_dest_dir+\"/frame{0}{1}_{2};{3}.png\"\n",
        "  source = src_dest_dir+\"/{0}_{1}.pt\"\n",
        "  img = torch.load(source.format(\"img\", i))\n",
        "  perturbed_img = torch.load(source.format(\"perturbed_img\", i))\n",
        "  mask = torch.load(source.format(\"mask\", i))\n",
        "  real_mask = torch.load(source.format(\"real_mask\", i))\n",
        "  perturbed_mask = torch.load(source.format(\"perturbed_mask\", i))\n",
        "  save_fig(dest.format('_clean', '_seg', i, alpha), img.clone(), mask, alpha=alpha) \n",
        "  save_fig(dest.format('_clean', '', i, alpha), img.clone())\n",
        "  save_fig(dest.format('_clean_real', '', i, alpha), img.clone(), real_mask, alpha=alpha)\n",
        "  save_fig(dest.format('_attacked', '_seg', i, alpha), perturbed_img.clone(), perturbed_mask, alpha=alpha) \n",
        "  save_fig(dest.format('_attacked', '', i, alpha), perturbed_img.clone()) \n",
        "\n",
        "\n",
        "def save_images_from_tensors(idxs=range(500), alpha=0.9):\n",
        "  \"\"\"\n",
        "  Given an iterable of indexes and alpha, saves the image with the given alpha according to the tensor\n",
        "  \"\"\"\n",
        "  for i in idxs:\n",
        "    print(\"saving img idx:\"+str(i)+\", with alpha:\"+str(alpha))\n",
        "    save_img_from_tensor(i, alpha=alpha)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwTz5exDcCWB"
      },
      "source": [
        "run_image_save = True\n",
        "if run_image_save == True:\n",
        "  required_alphas = [0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55, 0.6, 0.75, 0.9, 1]\n",
        "  for alpha in required_alphas:\n",
        "    save_images_from_tensors(range(len(val_loader)),alpha=alpha)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}