{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "challenge_detection_methods.ipynb",
   "private_outputs": true,
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "t2wkK9bMSAcy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#from tensorflow.keras.layers import *\n",
    "#from tensorflow.keras.models import Model\n",
    "#from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KorO33RGSEPn",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "today = datetime.date.today()\n",
    "d1 = today.strftime(\"%d-%m-%Y\")\n",
    "\n",
    "submission_predictions_filepath = '207632118_205644941.csv'\n",
    "\n",
    "RAW_DATA_PATH = 'FraudedRawData'\n",
    "\n",
    "PARTIAL_LABELS_PATH = 'challengeToFill.csv'\n",
    "\n",
    "LABELS_OUTPUT_PATH = 'submission_' + d1 + '_statistics.csv'\n",
    "\n",
    "MAX_NUM_OF_MALICIOUS = 10\n",
    "\n",
    "num_train_segments = 50\n",
    "\n",
    "total_num_users_to_train = 40\n",
    "\n",
    "last_idx_usr_train = 9\n",
    "\n",
    "hidden_layer_sizes = (512, 256, 128, 64, 32, 16, 8, 4, 2)\n",
    "max_num_of_iterations = 5000\n",
    "alpha = 0.0001\n",
    "random_state = 21\n",
    "loss_tolerance = 0.000000001\n",
    "\n",
    "num_of_segments = 150\n",
    "\n",
    "num_of_cmds_per_segment = 100\n",
    "\n",
    "max_num_of_ngrams = 10\n",
    "\n",
    "svd_flag = True\n",
    "\n",
    "num_of_features_svd = 100\n",
    "\n",
    "use_avg_statistics = True\n",
    "\n",
    "chosen_classifiers = ['MLPClassifier', 'GradientBoostingClassifier', 'DecisionTreeClassifier']"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "u9g4u84wSIeP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def results_predict(user_id, predicts_data, labels, file_path):\n",
    "    # accuracy: (tp + tn) / (p + n)\n",
    "    accuracy = accuracy_score(labels, predicts_data)\n",
    "    #print('Accuracy: %f' % accuracy)\n",
    "\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(labels, predicts_data)\n",
    "    #print('Precision: %f' % precision)\n",
    "\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(labels, predicts_data)\n",
    "    #print('Recall: %f' % recall)\n",
    "\n",
    "    # f1: 2 tp / (2 tp + fp + fn)\n",
    "    f1 = f1_score(labels, predicts_data)\n",
    "    #print('F1 score: %f' % f1)\n",
    "\n",
    "    auc = roc_auc_score(labels, predicts_data[:])\n",
    "    #print('ROC AUC: %f' % auc)\n",
    "\n",
    "    CM = confusion_matrix(labels, predicts_data)\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0]\n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "\n",
    "    dict_results_predict = {'userID': user_id,\n",
    "                            'date': datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"),\n",
    "                            'accuracy': accuracy,\n",
    "                            'TP': TP,\n",
    "                            'TN': TN,\n",
    "                            'FP': FP,\n",
    "                            'FN': FN,\n",
    "                            'precision': precision,\n",
    "                            'recall': recall,\n",
    "                            'f1': f1,\n",
    "                            'AUC': auc\n",
    "                            }\n",
    "\n",
    "    current_results_predict = pd.DataFrame([dict_results_predict])\n",
    "\n",
    "    try:\n",
    "        all_results = pd.read_csv(file_path, index_col=False)\n",
    "        all_results = all_results.append(current_results_predict)\n",
    "        all_results.to_csv(file_path, index=False)\n",
    "        print('added results to .csv, file shape is {}'.format(all_results.shape))\n",
    "    except FileNotFoundError:\n",
    "        current_results_predict.to_csv(file_path, index=False)\n",
    "        print('new .csv has been created')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "s-MlaCkXSQNg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def DataScaler(data):\n",
    "    '''\n",
    "    scaling the data into Z for normalizing!\n",
    "    @param data: data\n",
    "    @return:  Z scaled data\n",
    "    '''\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "    data = scaler.fit_transform(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def tfidf_vectorization_with_optional_svd(list_of_strings, index, ngram_range=(1, 1), svd_flag=True,\n",
    "                                          n_features=num_of_features_svd):\n",
    "    '''\n",
    "    Perform multiple tfidf vectorizations on the data.\n",
    "    @param list_of_strings: list of strings to vec\n",
    "    @param index: index to attach to returned DataFrame\n",
    "    @param ngram_range: range of ngrams to use\n",
    "    @param svd_flag: if to apply svd or not\n",
    "    @return df: vectorized pandas DataFrame\n",
    "    '''\n",
    "    tfidf = TfidfVectorizer(use_idf=False, norm=None, ngram_range=ngram_range)\n",
    "    scaled_matrix = DataScaler(tfidf.fit_transform(list_of_strings))\n",
    "\n",
    "    if svd_flag:\n",
    "        svd = TruncatedSVD(n_components=n_features, n_iter=10, random_state=356)\n",
    "        transformed = svd.fit_transform(scaled_matrix)\n",
    "        df = pd.DataFrame(transformed, index=index)\n",
    "    else:\n",
    "        df = pd.DataFrame(scaled_matrix, index=index, columns=tfidf.get_feature_names())\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def calc_statistics(transformed_df, segmented_data_dict, mode='train', calc_avg_of_statistic=False):\n",
    "    '''\n",
    "    create avg for each segment, and for each user(each segment for user -> each user)\n",
    "    two features ->\n",
    "    # -> 'avg_segment' = avg len of cmds in each segment, user based.\n",
    "    # -> 'total_avg' = avg len of cmds for each user\n",
    "    '''\n",
    "\n",
    "    def calc_len_segment_avgs(row):\n",
    "        curr_idx = str(int(row['userID'])) + \"_\" + str(int(row['segmentID']))\n",
    "        curr_cmds = segmented_data_dict[curr_idx]\n",
    "        curr_cmds_split = curr_cmds.split()\n",
    "        if len(curr_cmds_split) == 0:\n",
    "            return 0\n",
    "        curr_avg = sum(map(len, curr_cmds_split)) / len(curr_cmds_split)\n",
    "        return curr_avg\n",
    "\n",
    "    def calc_len_segment_median(row):\n",
    "        curr_idx = str(int(row['userID'])) + \"_\" + str(int(row['segmentID']))\n",
    "        curr_cmds = segmented_data_dict[curr_idx]\n",
    "        curr_cmds_split = curr_cmds.split()\n",
    "        if len(curr_cmds_split) == 0:\n",
    "            return 0\n",
    "        curr_median = np.median(list(map(len, curr_cmds_split)))\n",
    "        return curr_median\n",
    "\n",
    "    def calc_len_segment_std(row):\n",
    "        curr_idx = str(int(row['userID'])) + \"_\" + str(int(row['segmentID']))\n",
    "        curr_cmds = segmented_data_dict[curr_idx]\n",
    "        curr_cmds_split = curr_cmds.split()\n",
    "        if len(curr_cmds_split) == 0:\n",
    "            return 0\n",
    "        curr_std = np.std(list(map(len, curr_cmds_split)))\n",
    "        return curr_std\n",
    "\n",
    "    if mode == 'train':\n",
    "        num_to_repeat = num_train_segments\n",
    "    else:\n",
    "        num_to_repeat = num_of_segments - num_train_segments\n",
    "\n",
    "    transformed_df.loc[:, 'avg_segment'] = transformed_df.apply(calc_len_segment_avgs, axis=1)\n",
    "    if calc_avg_of_statistic == True:\n",
    "        avg_of_avgs = transformed_df.groupby('userID')['avg_segment'].mean()\n",
    "        repeated_avg_of_avgs = avg_of_avgs.iloc[np.arange(len(avg_of_avgs)).repeat(num_to_repeat)]\n",
    "        transformed_df.loc[:, 'total_avg'] = repeated_avg_of_avgs.values\n",
    "\n",
    "    #calculate median as above.\n",
    "    transformed_df.loc[:, 'median_segment'] = transformed_df.apply(calc_len_segment_median, axis=1)\n",
    "    if calc_avg_of_statistic == True:\n",
    "        avg_of_medians = transformed_df.groupby('userID')['median_segment'].mean()\n",
    "        repeated_avg_of_medians = avg_of_medians.iloc[np.arange(len(avg_of_medians)).repeat(num_to_repeat)]\n",
    "        transformed_df.loc[:, 'total_median_avg'] = repeated_avg_of_medians.values\n",
    "\n",
    "    #calculate standard deviation as above.\n",
    "    transformed_df.loc[:, 'std_segment'] = transformed_df.apply(calc_len_segment_std, axis=1)\n",
    "    if calc_avg_of_statistic == True:\n",
    "        avg_of_stds = transformed_df.groupby('userID')['std_segment'].mean()\n",
    "        repeated_avg_of_stds = avg_of_stds.iloc[np.arange(len(avg_of_stds)).repeat(num_to_repeat)]\n",
    "        transformed_df.loc[:, 'total_std_avg'] = repeated_avg_of_stds.values\n",
    "\n",
    "    return transformed_df\n",
    "\n",
    "\n",
    "def predict_segments(segmented_data_dict, use_svd_flag=True, ngrams=10):\n",
    "    if os.path.isfile(PARTIAL_LABELS_PATH):\n",
    "        print(\"partial labels file found.. Loading...\")\n",
    "        partial_labels_df = pd.read_csv(PARTIAL_LABELS_PATH)\n",
    "        #print(partial_labels_df.columns)\n",
    "        #print(partial_labels_df.columns[0])\n",
    "    else:\n",
    "        print(\"partial labels file not found.. Continuing...\")\n",
    "\n",
    "    users_segments_keys = segmented_data_dict.keys()\n",
    "    str_based_data_segments = segmented_data_dict.values()\n",
    "    transformed_df = tfidf_vectorization_with_optional_svd(str_based_data_segments, users_segments_keys,\n",
    "                                                           ngram_range=(1, ngrams), svd_flag=use_svd_flag)\n",
    "    #print(transformed_df)\n",
    "\n",
    "    transformed_df.loc[:, 'userID'] = transformed_df.index\n",
    "    transformed_df.loc[:, 'userID'] = transformed_df['userID'].apply(lambda x: int(x.split('_')[0]))\n",
    "    transformed_df.loc[:, 'segmentID'] = transformed_df.index\n",
    "    transformed_df.loc[:, 'segmentID'] = transformed_df['segmentID'].apply(lambda x: int(x.split('_')[1]))\n",
    "    transformed_df.sort_values(by=['userID', 'segmentID'], inplace=True)\n",
    "\n",
    "    tfidf_until_seg_train = transformed_df[(transformed_df['segmentID'] < num_train_segments)]\n",
    "\n",
    "    if use_avg_statistics == True:\n",
    "        tfidf_until_seg_train = calc_statistics(tfidf_until_seg_train, segmented_data_dict, 'train')\n",
    "\n",
    "    num_users_to_train = len(set(tfidf_until_seg_train['userID']))  #get number of users, by set of IDS\n",
    "\n",
    "    if total_num_users_to_train != num_users_to_train:\n",
    "        print(\"Not Equal num of users to train! something not right with the input... Exiting!\")\n",
    "        exit()\n",
    "\n",
    "    tfidf_train_matrix = tfidf_until_seg_train.drop(['userID', 'segmentID'], axis=1)\n",
    "    print(tfidf_train_matrix)\n",
    "\n",
    "    'One versus All - [0](benign) just for current idx, the rest are [1](malicious)'\n",
    "\n",
    "    for index in tqdm(range(num_users_to_train)):\n",
    "\n",
    "        first_rest_labels = np.ones(num_train_segments * index,\n",
    "                                    dtype=int)  #count as malicious, to fill before index benign if needed\n",
    "        current_idx_labels = np.zeros(num_train_segments, dtype=int)  #count as benign\n",
    "        rest_labels = np.ones(num_train_segments * (num_users_to_train - index - 1), dtype=int)  #count as malicious\n",
    "\n",
    "        train_labels = np.concatenate((first_rest_labels, current_idx_labels, rest_labels), axis=0)\n",
    "\n",
    "        user_idx_segs_val_test = transformed_df[\n",
    "            (transformed_df['userID'] == index) & (transformed_df['segmentID'] >= num_train_segments)].copy()\n",
    "\n",
    "        if use_avg_statistics == True:\n",
    "            user_idx_segs_val_test = calc_statistics(user_idx_segs_val_test, segmented_data_dict, 'test')\n",
    "\n",
    "        segs_val_test = user_idx_segs_val_test.drop(['userID', 'segmentID'], axis=1)\n",
    "\n",
    "        #train\n",
    "\n",
    "        models = []\n",
    "\n",
    "        for model_name in chosen_classifiers:\n",
    "\n",
    "            if model_name == 'MLPClassifier':\n",
    "\n",
    "                model = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, max_iter=max_num_of_iterations,\n",
    "                                      alpha=alpha,\n",
    "                                      solver='adam', verbose=False, random_state=random_state, tol=loss_tolerance)\n",
    "\n",
    "            elif model_name == 'GradientBoostingClassifier':\n",
    "\n",
    "                model = GradientBoostingClassifier(verbose=False, random_state=random_state, tol=loss_tolerance)\n",
    "\n",
    "\n",
    "            elif model_name == 'DecisionTreeClassifier':\n",
    "\n",
    "                model = DecisionTreeClassifier(criterion='gini', random_state=random_state)\n",
    "\n",
    "            model.fit(tfidf_train_matrix, train_labels)\n",
    "\n",
    "            models.append(model)\n",
    "\n",
    "        #predict\n",
    "\n",
    "        print(\"****************\")\n",
    "\n",
    "        print(\"User idx:\" + str(index))\n",
    "\n",
    "        eclf = VotingClassifier(estimators=[('mplc', models[0]), ('abc', models[1]), ('dtc', models[2])], voting='soft')\n",
    "\n",
    "        eclf.fit(tfidf_train_matrix, train_labels)\n",
    "\n",
    "        #predict\n",
    "\n",
    "        pred_for_user = eclf.predict(segs_val_test)\n",
    "\n",
    "        if sum(pred_for_user) > MAX_NUM_OF_MALICIOUS:  #change more malicious than MAX_NUM_OF_MALICIOUS to benign, besides top@MAX_NUM_OF_MALICIOUS\n",
    "\n",
    "            result_probs = eclf.predict_proba(segs_val_test)\n",
    "\n",
    "            top_indexes = result_probs[:, 0].argsort()[:MAX_NUM_OF_MALICIOUS]\n",
    "\n",
    "            pred_for_user_after_top = np.zeros(len(pred_for_user), dtype=int)\n",
    "            pred_for_user_after_top[top_indexes] = 1\n",
    "\n",
    "        prediction_for_user = np.array(pred_for_user_after_top).astype(np.int)\n",
    "\n",
    "        print(prediction_for_user)\n",
    "\n",
    "        if index > last_idx_usr_train:  #need to add to submission since it test,changing PARTIAL_LABELS_PATH file\n",
    "            return\n",
    "            cmd_id = num_train_segments * num_of_cmds_per_segment  #starting from 5000 (50 segments * 100 cmds per segment)\n",
    "            for pred in prediction_for_user:\n",
    "                row_idx = 'User' + str(index)\n",
    "                clmn_idx = str(cmd_id) + \"-\" + str(cmd_id + num_of_cmds_per_segment)\n",
    "                partial_labels_df.at[row_idx, clmn_idx] = pred\n",
    "                cmd_id += num_of_cmds_per_segment\n",
    "\n",
    "        else:  #training evaluation!\n",
    "\n",
    "            user_id = 'User' + str(index)\n",
    "            column_idx_in_df = partial_labels_df.columns[0]\n",
    "            desired_row = partial_labels_df.loc[(partial_labels_df[column_idx_in_df] == user_id)].values\n",
    "            labels_preds = desired_row[0][\n",
    "                           num_train_segments + 1:]  #num_train_segments +1 because row started with value UserX, skip num_train_segments to get val+test\n",
    "            labels_preds = np.array(labels_preds).astype(np.int)\n",
    "\n",
    "            results_predict(user_id, prediction_for_user, labels_preds, LABELS_OUTPUT_PATH)\n",
    "\n",
    "    partial_labels_df.to_csv(submission_predictions_filepath, index=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iJC_jAG8TllD",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def make_average_statistics(path, num_users_to_count):\n",
    "    try:\n",
    "        all_results = pd.read_csv(path, index_col=False)[-num_users_to_count:]\n",
    "        avg_results_predict_dict = {'date': datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"),\n",
    "                                    'accuracy': all_results['accuracy'].mean(),\n",
    "                                    'TP': all_results['TP'].mean(),\n",
    "                                    'TN': all_results['TN'].mean(),\n",
    "                                    'FP': all_results['FP'].mean(),\n",
    "                                    'FN': all_results['FN'].mean(),\n",
    "                                    'precision': all_results['precision'].mean(),\n",
    "                                    'recall': all_results['recall'].mean(),\n",
    "                                    'f1': all_results['f1'].mean(),\n",
    "                                    'AUC': all_results['AUC'].mean(),\n",
    "                                    'userID': 999999\n",
    "                                    }\n",
    "        avg_results_predict = pd.DataFrame([avg_results_predict_dict])\n",
    "        all_results = all_results.append(avg_results_predict)\n",
    "        all_results.to_csv(path, index=False)\n",
    "        print('added average results to statistics.csv')\n",
    "    except Exception:\n",
    "        print(\"Not found necassery information to make average... Exiting!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wpZ9_rW9QMO2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def load_dataset(RAW_DATA_PATH, num_of_segments, num_of_cmds_per_segment):\n",
    "    # segmented_data_dict -> {'User-ID_Segment-ID':[str composed of commands which are seperated by spaces]}\n",
    "    segmented_data_dict = {}\n",
    "    for filename in os.listdir(RAW_DATA_PATH):\n",
    "        user_id = filename[len('User'):]\n",
    "        filePath = RAW_DATA_PATH + '/' + filename\n",
    "        with open(filePath) as fp:\n",
    "            data = ''\n",
    "\n",
    "            for i in range(num_of_segments):\n",
    "                for j in range(num_of_cmds_per_segment):  # 100 commands each - 1 segment\n",
    "                    line = fp.readline()\n",
    "                    if line:\n",
    "                        data += line[:-1] + ' '  # remove \\n and add space\n",
    "                    else:\n",
    "                        print(\"on file:\" + filename + \", there is a bug on segment: \" + str(\n",
    "                            i + 1) + \", missing command:\" + str(j) + \"!\")\n",
    "                segmented_data_dict[user_id + \"_\" + str(i)] = data\n",
    "                data = ''\n",
    "    return segmented_data_dict"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UXxrUg75p5SY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "segmented_data_dict = load_dataset(RAW_DATA_PATH, num_of_segments, num_of_cmds_per_segment)\n",
    "\n",
    "predict_segments(segmented_data_dict, svd_flag, max_num_of_ngrams)\n",
    "\n",
    "num_users_to_calc_avg = last_idx_usr_train + 1\n",
    "\n",
    "make_average_statistics(LABELS_OUTPUT_PATH, num_users_to_calc_avg)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}