{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from vit_transformer import VitTransformer\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, Normalize, RandomHorizontalFlip,RandomResizedCrop, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layers = 3\n",
    "\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\" +str(device))\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "smoothing_rate = 0.1\n",
    "\n",
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cifar10 \n",
    "train_set, test_set = load_dataset('cifar10', split=['train', 'test'])\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "\n",
    "image_h_w, img_channels, patch_h_w = 224, 3, 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_transform_steps = Compose(\n",
    "        [\n",
    "            RandomResizedCrop(feature_extractor.size),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "def train_transforms(images):\n",
    "    transformed_images =  [_train_transform_steps(curr_image['img'].convert(\"RGB\")) for curr_image in images]\n",
    "    return transformed_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_func(images):\n",
    "    labels = torch.tensor([image_t[\"label\"] for image_t in images])\n",
    "    return {\"images\": images, \"labels\": labels}\n",
    "\n",
    "train_loader = DataLoader(train_set, collate_fn=collate_func, batch_size=batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = VitTransformer(image_h_w, img_channels, patch_h_w, embed_dim, num_heads, encoder_layers, num_classes).to(device)\n",
    "adam_opt = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "ce_loss = torch.nn.CrossEntropyLoss(label_smoothing=smoothing_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(transformer, train_loader):\n",
    "    transformer.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        batches_total_loss = 0\n",
    "        batches_total_size = 0\n",
    "\n",
    "        for i, batch_data in enumerate(train_loader):\n",
    "\n",
    "            transformed_images = torch.stack(train_transforms(batch_data['images']))\n",
    "            transformed_images = transformed_images.to(device)\n",
    "            batch_size = transformed_images.shape[0]\n",
    "\n",
    "            labels = batch_data['labels']\n",
    "            labels = labels.to(device)\n",
    "            labels = labels.contiguous().view(-1)  # dims: [batch_size * 1]\n",
    "\n",
    "            preds = transformer(transformed_images)\n",
    "\n",
    "            adam_opt.zero_grad()\n",
    "\n",
    "            loss = ce_loss(preds, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            adam_opt.step()\n",
    "\n",
    "            batches_total_loss += loss.item() * batch_size\n",
    "            batches_total_size += batch_size\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch: [{epoch}] Batch:[{i}/{len(train_loader)}]\\tLoss: {batches_total_loss/batches_total_size:.3f}\")\n",
    "\n",
    "        state = {'epoch': epoch, 'model': transformer, 'optimizer': adam_opt}\n",
    "        torch.save(state, 'vit_model_epoch_' + str(epoch) + '.pth')\n",
    "        print(\"saved model on epoch: \"+str(epoch))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(transformer, train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
