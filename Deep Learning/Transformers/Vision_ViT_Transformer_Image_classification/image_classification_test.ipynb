{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from vit_transformer import VitTransformer\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import CenterCrop, Compose, Normalize, Resize, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "encoder_layers = 6\n",
    "\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\" + str(device))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load cifar10 \n",
    "train_set, test_set = load_dataset('cifar10', split=['train', 'test'])\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "\n",
    "image_h_w, img_channels, patch_h_w = 224, 3, 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_val_test_transforms_steps = Compose(\n",
    "    [\n",
    "        Resize(feature_extractor.size),\n",
    "        CenterCrop(feature_extractor.size),\n",
    "        ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "\n",
    "def val_test_transforms(images):\n",
    "    transformed_images = [_val_test_transforms_steps(curr_image['img'].convert(\"RGB\")) for curr_image in images]\n",
    "    return transformed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def collate_func(images):\n",
    "    labels = torch.tensor([image_t[\"label\"] for image_t in images])\n",
    "    return {\"images\": images, \"labels\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model of epoch: 1\n"
     ]
    }
   ],
   "source": [
    "epoch_model_to_load = 1\n",
    "load_model = torch.load('vit_model_epoch_' + str(epoch_model_to_load) + '.pth', map_location=device)\n",
    "model = load_model['model']\n",
    "print(\"load model of epoch: \" + str(epoch_model_to_load))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = len(test_set)\n",
    "test_loader = DataLoader(test_set, collate_fn=collate_func, batch_size=batch_size, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test(model, test_loader, metrics):\n",
    "    model.eval()\n",
    "    metrics_results = []\n",
    "    for i, batch_data in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "            transformed_images = torch.stack(val_test_transforms(batch_data['images']))\n",
    "            transformed_images = transformed_images.to(device)\n",
    "\n",
    "            labels = batch_data['labels']\n",
    "            labels = labels.to(device)\n",
    "            labels = labels.contiguous().view(-1)  # dims: [batch_size * 1]\n",
    "\n",
    "            preds = model(transformed_images)\n",
    "            _, idxes = torch.max(preds, dim=1)\n",
    "\n",
    "            for met_criterion in metrics:\n",
    "                metric = load_metric(met_criterion)\n",
    "                metrics_results.append(metric.compute(predictions=idxes, references=labels))\n",
    "\n",
    "    return metrics_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metrics = ['accuracy']\n",
    "metrics_results = test(model, test_loader, metrics)\n",
    "print(metrics_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}