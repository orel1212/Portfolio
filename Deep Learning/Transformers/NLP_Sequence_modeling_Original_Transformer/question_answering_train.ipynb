{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer import Transformer, AdamWithWarpUp, LossCE\n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "convs_path = './conversations.txt'\n",
    "lines_path = './lines.txt'\n",
    "encoded_qa_path = './pairs_encoded.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "seq_len = 30\n",
    "encoder_layers = 6\n",
    "decoder_layers = 6\n",
    "\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\" + str(device))\n",
    "epochs = 2\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "pad_token = 0\n",
    "\n",
    "vocab_size = len(tokenizer)  # FROM GPT2\n",
    "print(f\"vocab_tokenizer size:{vocab_size}\")\n",
    "smoothing_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if Path(convs_path).exists():\n",
    "    with open(convs_path, 'r') as c:\n",
    "        convs = c.readlines()\n",
    "else:\n",
    "    print(\"Not found conversation file... Exiting!\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if Path(lines_path).exists():\n",
    "    with open(lines_path, 'r') as l:\n",
    "        lines = l.readlines()\n",
    "else:\n",
    "    print(\"Not found lines file... Exiting!\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lines_id_data_dict = {}\n",
    "for line in lines:\n",
    "    line_info = line.split(\" +++$+++ \")\n",
    "    line_id = line_info[0]\n",
    "    line_data = line_info[-1]\n",
    "    lines_id_data_dict[line_id] = line_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def remove_punctuations(p_str):\n",
    "    punctuations = '''<>./?@#$%^&*_~!()-[]{};:'\"\\,'''  #save spaces\n",
    "    new_str = \"\"\n",
    "    for char in p_str:\n",
    "        if char not in punctuations:\n",
    "            new_str = new_str + char\n",
    "    new_str = new_str.lower()  #avoid upper and lower difference\n",
    "    return new_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "question_answer_pairs = []\n",
    "for conv in convs:\n",
    "    line_ids_str = conv.split(\" +++$+++ \")[-1]\n",
    "    line_ids = eval(line_ids_str)  #get line id via eval 'L195' -> L195\n",
    "    for i in range(len(line_ids) - 1):\n",
    "        curr_line = lines_id_data_dict[line_ids[i]].strip()\n",
    "        next_line = lines_id_data_dict[line_ids[i + 1]].strip()\n",
    "        question = remove_punctuations(curr_line)\n",
    "        answer = remove_punctuations(next_line)\n",
    "        question_answer_pairs.append([question, answer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def encode_question_tokenizer(question_words, seq_len, tokenizer, pad_token):\n",
    "    #enc_q_without_pad = tokenizer(question_words)['input_ids'][:seq_len]\n",
    "    enc_q_without_pad = tokenizer.encode(question_words, add_special_tokens=True, truncation=True, max_length=seq_len)\n",
    "    padding_len = seq_len - len(enc_q_without_pad)\n",
    "    enc_q = enc_q_without_pad + [pad_token] * padding_len\n",
    "    return enc_q\n",
    "\n",
    "\n",
    "def encode_answer_tokenizer(answer_words, seq_len, tokenizer, pad_token):\n",
    "    #enc_a_without_pad = tokenizer(answer_words)['input_ids'][:seq_len]\n",
    "    enc_a_without_pad = tokenizer.encode(answer_words, add_special_tokens=True, truncation=True, max_length=seq_len)\n",
    "    padding_len = seq_len - len(enc_a_without_pad)\n",
    "    enc_a = [tokenizer.bos_token_id] + enc_a_without_pad + [tokenizer.eos_token_id] + [pad_token] * padding_len\n",
    "    return enc_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "encoded_qa = []\n",
    "for qa_pair in question_answer_pairs:\n",
    "    e_question = encode_question_tokenizer(qa_pair[0], seq_len, tokenizer, pad_token)\n",
    "    e_answer = encode_answer_tokenizer(qa_pair[1], seq_len, tokenizer, pad_token)\n",
    "    encoded_qa.append([e_question, e_answer])\n",
    "\n",
    "with open(encoded_qa_path, 'w') as qa_p:\n",
    "    json.dump(encoded_qa, qa_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(embed_dim, num_heads, encoder_layers, decoder_layers, vocab_size, seq_len).to(device)\n",
    "adam_opt = torch.optim.Adam(transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "warmup_steps = 4000\n",
    "adam_warmup_opt = AdamWithWarpUp(adam_opt, embed_dim, warmup_steps)\n",
    "ce_loss = LossCE(smoothing_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_mask(input_seq, mask_flag='q'):\n",
    "    curr_seq_len = input_seq.size(-1)\n",
    "    input_mask = input_seq != 0\n",
    "    if mask_flag == 'q':  #no need to mask future, as it goes thorugh encoder\n",
    "        question_mask = input_mask.unsqueeze(1).unsqueeze(1)\n",
    "        mask = question_mask  # required dims : [batch_size, 1, 1, seq_len]\n",
    "    elif mask_flag == 'a':  # need to mask future, as it goes thorugh decoder, via triu-transpose\n",
    "        answer_mask = input_mask.unsqueeze(1)\n",
    "        answer_mask = answer_mask & torch.triu(torch.ones(curr_seq_len, curr_seq_len)).transpose(0, 1).type(\n",
    "            dtype=torch.uint8).unsqueeze(0).type_as(answer_mask.data)\n",
    "        answer_mask = answer_mask.unsqueeze(1)\n",
    "        mask = answer_mask  # required dims: [batch_size, 1, seq_len, seq_len]\n",
    "    else:  #mask_flag == 't' # need only for loss\n",
    "        mask = input_mask  # required dims: [batch_size, seq_len]\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class QuestionAnswerDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        if Path(encoded_qa_path).exists():\n",
    "            with open(encoded_qa_path, 'r') as qa_r:\n",
    "                self.qa_pairs = json.load(qa_r)\n",
    "        else:\n",
    "            print(\"Not found encoded question-answer pairs file... Exiting!\")\n",
    "            exit()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.qa_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.qa_pairs[idx][0]\n",
    "        answer = self.qa_pairs[idx][1]\n",
    "        question_t = torch.LongTensor(question)\n",
    "        answer_t = torch.LongTensor(answer)\n",
    "        return question_t, answer_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "train_loader = torch.utils.data.DataLoader(QuestionAnswerDataset(),\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    transformer.train()\n",
    "\n",
    "    batches_total_loss = 0\n",
    "    batches_total_size = 0\n",
    "\n",
    "    for i, (question, answer) in enumerate(train_loader):\n",
    "\n",
    "        adam_warmup_opt.optimizer.zero_grad()\n",
    "\n",
    "        batch_size = question.shape[0]\n",
    "\n",
    "        question = question.to(device)\n",
    "        answer = answer.to(device)\n",
    "\n",
    "        answer_input = answer[:, :-1]\n",
    "        answer_target = answer[:, 1:]\n",
    "\n",
    "        question_mask = create_mask(question, mask_flag='q').to(device)\n",
    "        answer_input_mask = create_mask(answer_input, mask_flag='a').to(device)\n",
    "        answer_target_mask = create_mask(answer_target, mask_flag='t').to(device)\n",
    "\n",
    "        preds = transformer(question, question_mask, answer_input, answer_input_mask)\n",
    "\n",
    "        loss = ce_loss(preds, answer_target, answer_target_mask)\n",
    "\n",
    "        loss.backward()\n",
    "        adam_warmup_opt.step()\n",
    "\n",
    "        batches_total_loss += loss.item() * batch_size\n",
    "        batches_total_size += batch_size\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch: [{epoch}] Batch:[{i}/{len(train_loader)}]\\tLoss: {batches_total_loss / batches_total_size:.3f}\")\n",
    "\n",
    "    state = {'epoch': epoch, 'model': transformer, 'optimizer': adam_warmup_opt}\n",
    "    torch.save(state, 'model_epoch_' + str(epoch) + '.pth')\n",
    "    print(\"saved model on epoch: \" + str(epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}